# AgentStack - LLM Chat Application

A complete learning project demonstrating how to build a modern AI chat application using local LLM models with Ollama, LiteLLM proxy, and Streamlit frontend.

## ğŸš€ Current Status

**Active Phase**: Docker-Only Setup (Phase 1C)

### Completed âœ…
- Ollama native setup documented
- Docker Compose configuration created
- Container 1 (qwen2.5:0.5b) - Ready on Port 11435
- Native Ollama removed & cleanup complete

### In Progress â³
- Container 2 (phi3:mini) downloading (10-15 mins)

### Next Up â¡ï¸
- Phase 2: LiteLLM Backend Server

---

## ğŸ—ï¸ Current Architecture

### Docker-Only Setup (ACTIVE)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STREAMLIT (Coming in Phase 3)          â”‚
â”‚              Port 8501                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LITELLM (Coming in Phase 2)              â”‚
â”‚              Port 8000                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ DOCKER 1       â”‚  â”‚  DOCKER 2        â”‚
    â”‚ Port 11435     â”‚  â”‚  Port 11436      â”‚
    â”‚ qwen2.5:0.5b   â”‚  â”‚  phi3:mini       â”‚
    â”‚ (397 MB) âœ…     â”‚  â”‚  (2.3 GB) â³      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Communication Flow

1. **User Input** â†’ Streamlit Frontend (Port 8501)
   - User types a message and selects a model
   - Frontend sends HTTP request to LiteLLM

2. **Request Processing** â†’ LiteLLM Proxy (Port 8000)
   - Receives request from Streamlit
   - Routes to appropriate Ollama model
   - Maintains request/response logs

3. **LLM Processing** â†’ Ollama Service (Port 11434)
   - Executes inference on selected model
   - Streams response back to LiteLLM
   - Returns generated text

4. **Response Display** â†’ Streamlit Frontend
   - Displays streamed response in real-time
   - Shows token count and model info
   - Maintains conversation history

## ğŸ“ Project Structure

```
AgentStack/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ litellm_server.py       # LiteLLM proxy server
â”‚   â”œâ”€â”€ config.py               # Configuration settings
â”‚   â””â”€â”€ requirements.txt         # Backend dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ streamlit_app.py         # Streamlit chat interface
â”‚   â””â”€â”€ requirements.txt         # Frontend dependencies
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_ollama.sh          # Ollama installation script
â”‚   â”œâ”€â”€ download_models.sh       # Download 3 LLM models
â”‚   â””â”€â”€ run_project.sh           # Start all services
â”œâ”€â”€ docker-compose.yml           # Container orchestration
â”œâ”€â”€ pyproject.toml               # Project metadata
â”œâ”€â”€ setup.md                     # Setup instructions
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Technology Stack

### Ollama
- **Purpose**: Local LLM model serving
- **Port**: 11434
- **Models**: llama3.2:1b, qwen2.5:0.5b, phi3:mini
- **Why**: Runs entirely on local hardware, no external API calls

### LiteLLM
- **Purpose**: Unified LLM API proxy
- **Port**: 8000
- **Features**: Standardized OpenAI-compatible API, easy model switching
- **Why**: Simplifies API calls, makes switching models seamless

### Streamlit
- **Purpose**: Interactive web UI for chat
- **Port**: 8501
- **Features**: Real-time response streaming, conversation history
- **Why**: Easy to build, fast prototyping, great for demos

## ğŸ”„ Data Flow Example

```
User Message: "Hello"
      â†“
[Streamlit] Makes POST request to LiteLLM:
  {
    "model": "ollama/llama3.2:1b",
    "messages": [{"role": "user", "content": "Hello"}]
  }
      â†“
[LiteLLM] Routes to Ollama and forwards:
  http://localhost:11434/api/generate
      â†“
[Ollama] Generates response with selected model
      â†“
[LiteLLM] Streams response back to Streamlit
      â†“
[Streamlit] Displays response in chat UI
```

## ğŸ“‹ Step-by-Step Implementation Plan

### Phase 1: âœ… Ollama Setup (COMPLETE)
- [x] Install Ollama in WSL Ubuntu
- [x] Start Ollama service
- [x] Download 3 LLM models
- [x] Verify models are accessible

### Phase 1B: âœ… Docker Container Setup (COMPLETE)
- [x] Create docker-compose.yml with 2 lightweight containers
- [x] Start Docker containers
- [x] Download qwen2.5:0.5b to Container 1
- [x] Download phi3:mini to Container 2 (â³ in progress)

### Phase 1C: âœ… Cleanup & Docker-Only Setup (ACTIVE)
- [x] Remove native Ollama service
- [x] Delete all native models (freed 3.9 GB)
- [x] Keep ONLY Docker containers
- [x] Verify docker-only access
- â³ Wait for phi3:mini download

### Phase 2: LiteLLM Backend (NEXT)
- [ ] Create LiteLLM server configuration
- [ ] Set up model routing (both containers)
- [ ] Create API endpoints
- [ ] Test with curl/Postman

### Phase 3: Streamlit Frontend (COMING)
- [ ] Create chat interface
- [ ] Add model selector
- [ ] Implement message history
- [ ] Add streaming responses

### Phase 4: Integration & Testing (COMING)
- [ ] Connect Streamlit to LiteLLM
- [ ] Test end-to-end flow
- [ ] Add error handling
- [ ] Performance testing

## âœ… Prerequisites

- WSL Ubuntu 24.04
- Python 3.10+
- Docker (optional, for containerization)
- 4GB+ RAM (for running models)
- Internet connection (for initial setup)

## ğŸ“ Notes

- All services run locally without external API calls
- Models run on CPU (can be slow) or GPU (if available)
- Memory usage depends on model size and concurrent requests
- This is a learning project focused on architecture and integration

